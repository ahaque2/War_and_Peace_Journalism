{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f741acad-054e-443c-9fed-c95045392eee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code to use custom features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de126909-a4fa-444d-91ff-87909530c451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Change the variable model_name to whichever model you wish to use. Ensure to make other relevant changes (some models may not require any change). \n",
    "### The required changes have to be made inside LanguageModel class, change self.model = '' to appropriate model initialization, and in intializing the tokenizer.\n",
    "### Current code includes POS, and entity mentions as features. Additinally it uses projections of contextual embeddings to the moral vector subspace as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d017a-68b6-454d-9355-07922322be87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e41107-1e6b-4462-ba81-c66e87f9ce84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_entities(sentences: pd.Series) -> pd.Series:\n",
    "    def mask(sentence):\n",
    "        doc = nlp(sentence)\n",
    "        masked = sentence\n",
    "        for ent in doc.ents:\n",
    "            masked = masked.replace(ent.text, \"<mask>\")\n",
    "        return masked\n",
    "    return sentences.apply(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531a671-439c-45a1-8393-e94408e40a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(attr):\n",
    "    \n",
    "    df = pd.read_csv(f'data/Label_{attr}.csv')\n",
    "    df['headline'] = mask_entities(df['headline'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# df = get_data('1')\n",
    "# df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2fd95-b28b-4eef-a2be-107a60bc18f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings(wordlist, layer = 'mp', l = 1):\n",
    "\n",
    "    embeddings = []\n",
    "    \n",
    "    model_name = 'bert-base-cased'\n",
    "    model = AutoModel.from_pretrained(model_name, output_hidden_states = True, output_attentions = True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for word in wordlist:\n",
    "    \n",
    "        tokenized_inputs = tokenizer(word, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        emb = None\n",
    "\n",
    "        # Get BERT model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_inputs)\n",
    "            \n",
    "        # print(outputs.keys())\n",
    "\n",
    "        if layer == None:\n",
    "            first_layer = outputs.hidden_states[l]\n",
    "            emb = torch.mean(first_layer, dim=1).detach().cpu().numpy()\n",
    "\n",
    "        elif layer == 'ls':\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            emb = torch.mean(last_hidden_states, dim=1).detach().cpu().numpy()\n",
    "            \n",
    "        elif layer == 'po':\n",
    "            emb = outputs.pooler_output.detach().cpu().numpy()\n",
    "\n",
    "        else:\n",
    "            # print(\"Here \", outputs.keys())\n",
    "            hs = outputs.hidden_states\n",
    "            emb = torch.stack([x.mean(axis = 1) for x in hs]).mean(axis = 0).detach().cpu().numpy()\n",
    "            \n",
    "        embeddings.append(np.squeeze(emb))\n",
    "         \n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "# lex_df = pd.read_csv('/home/ahaque2/resources/NRC-VAD-Lexicon/NRC-VAD-Lexicon.csv')\n",
    "lex_df = pd.read_csv('data/eMFD_wordlist.csv')\n",
    "all_words = lex_df.word.tolist()\n",
    "\n",
    "emb = get_word_embeddings(all_words)\n",
    "\n",
    "emb_dict = dict()\n",
    "for word, e in zip(all_words, emb):\n",
    "    emb_dict[word] = e\n",
    "    \n",
    "model_name = 'bert-base-cased'\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states = True, output_attentions = True)\n",
    "    \n",
    "from Word_Pairs import Word_Pairs\n",
    "Word_Pairs_Class = Word_Pairs(model, emb_dict, lex_df)\n",
    "\n",
    "def get_subspace(high, low, num_keep, attr):\n",
    "    \n",
    "    word_pairs = Word_Pairs_Class.get_word_pairs_with_scores(high, low, num_keep, attr)\n",
    "\n",
    "    emb1 = [emb_dict[w] for w in word_pairs.Word1.tolist()]\n",
    "    emb2 = [emb_dict[w] for w in word_pairs.Word2.tolist()]\n",
    "\n",
    "    subspace, subspace_10 = Word_Pairs_Class.get_subspace(emb1, emb2, 10)\n",
    "\n",
    "    return subspace_10\n",
    "\n",
    "\n",
    "params = [(275, 475, 175, 'care_sent'), (350, 475, 325, 'fairness_sent'), (300, 375, 175, 'loyalty_sent'), (350, 475, 275, 'authority_sent'), \n",
    "          (275, 375, 225, 'sanctity_sent')]\n",
    "\n",
    "subspace = []\n",
    "for p in params:\n",
    "    sub = get_subspace(p[0], p[1], p[2], p[3])\n",
    "    subspace.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928d9aa-f7b1-4701-ab81-cbf8d1d7bb61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b3db5-4517-4040-89ec-131297697671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "# Load spaCy NLP model for feature extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, num_class, bert_model_name=model_name, feature_dim=10, dep_embed_dim=10):\n",
    "        super(CustomBERT, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name, output_hidden_states = True, output_attentions = True)\n",
    "        self.dep_embedding = nn.Embedding(10, dep_embed_dim)  # Adjust vocab size\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + feature_dim + dep_embed_dim + 50, num_class)\n",
    "        # self.classifier = nn.Linear(self.bert.config.hidden_size, num_class)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, features, dep_indices, sub_features):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # last_hidden_state_cls = outputs.last_hidden_state[:, 0, :]  # CLS token output\n",
    "        hs = outputs.hidden_states\n",
    "        emb = torch.stack([x.mean(axis = 1) for x in hs]).mean(axis = 0)\n",
    "\n",
    "        # Get dependency embeddings\n",
    "        dep_embeds = self.dep_embedding(dep_indices).mean(dim=1)\n",
    "\n",
    "        combined = torch.cat((emb, sub_features, features.float(), dep_embeds), dim=1)\n",
    "        # print(combined.shape)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cea419-1c66-419c-b6d1-48c42d6e8ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(text, entity_vocab, max_len=10):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = [sum(1 for token in doc if token.pos_ == tag) for tag in [\"NOUN\", \"VERB\", \"ADJ\"]]\n",
    "    dep_counts = [sum(1 for token in doc if token.dep_ == dep) for dep in [\"nsubj\", \"dobj\", \"pobj\"]]\n",
    "    entity_indices = [entity_vocab.get(ent.label_, 0) for ent in doc.ents]\n",
    "    \n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    sentiment_scores = [sentiment_scores[x] for x in sentiment_scores]\n",
    "    # Pad or truncate entity indices\n",
    "    entity_indices = entity_indices[:max_len] + [0] * (max_len - len(entity_indices))\n",
    "    \n",
    "    return torch.tensor(pos_counts + dep_counts + sentiment_scores, dtype=torch.float32), torch.tensor(entity_indices, dtype=torch.long)\n",
    "\n",
    "entity_vocab = {\"PERSON\": 1, \"ORG\": 2, \"GPE\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f1487-e068-428d-a218-9b6af6da02ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(sent, labels, feat1, feat2, sub_feat):\n",
    "    \n",
    "    tokens = tokenizer(sent, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    # tokens = tokenizer(sent, return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    # print(labels)\n",
    "    # sys.exit()\n",
    "    data = TensorDataset(tokens['input_ids'], tokens['attention_mask'], labels, feat1, feat2, sub_feat)\n",
    "\n",
    "    return DataLoader(data, batch_size = 64)\n",
    "\n",
    "def get_perf(dataloader):\n",
    "    \n",
    "    total_val_f1 = 0\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # emb = torch.tensor([emb])\n",
    "        input_ids, attention_mask, labels, feat1, feat2, sub_feat = [x.to(device) for x in batch]\n",
    "        # print(tok)\n",
    "        labels = labels.float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask, feat1, feat2, sub_feat)\n",
    "            \n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        total_val_f1 += f1_score(labels.cpu(), preds.cpu(), average = 'weighted')\n",
    "        \n",
    "    avg_val_f1 = total_val_f1 / len(dataloader)\n",
    "    \n",
    "    return avg_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc7a01-c6cd-4afd-97b0-c3385dcb643f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# model_name = 'bert-base-cased'\n",
    "# model_name = 'roberta-base'\n",
    "# model_name = 'daodao/ConflictBERT'\n",
    "# model_name = 'snowood1/ConfliBERT-scr-cased'\n",
    "# model_name = 'facebook/bart-base'\n",
    "# model_name = 'answerdotai/ModernBERT-base'\n",
    "# model_name = 'meta-llama/Llama-3.2-3B'\n",
    "# model_name = 'answerdotai/ModernBERT-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# siamese_training = 'verb_subspace'\n",
    "# 'head_cont_and_verb_subspace'\n",
    "# verb_subspace\n",
    "# 'simple_contrastive'\n",
    "\n",
    "# for siamese_training in ['verb_subspace', 'head_cont_and_verb_subspace', 'simple_contrastive']:\n",
    "\n",
    "siamese_training = None\n",
    "# attr = '4c'\n",
    "# for attr in ['1', '2a', '2b', '3a', '3b', '4a', '4b', '4c']:\n",
    "for attr in ['emotive', 'effects', 'source', 'context', 'villain', 'victim']:\n",
    "        # ['1', '4a', '4b']:\n",
    "        # \n",
    "        # for attr in ['1', '3a', '3b']:\n",
    "    # data = get_data(attr)\n",
    "    data = pd.read_csv(f'data/Label_{attr}.csv')\n",
    "    X_train, X_test, train_label, test_label = train_test_split(data.headline, data.label, test_size=0.2, stratify = data.label, random_state = 42)\n",
    "\n",
    "    train_features = [extract_features(t, entity_vocab) for t in X_train.tolist()]\n",
    "    pos_features_train = torch.stack([x[0] for x in train_features], dim = 0)\n",
    "    dep_features_train = torch.stack([x[1] for x in train_features], dim = 0)\n",
    "    \n",
    "    emb_train = get_word_embeddings(X_train.tolist())\n",
    "    sub_feat = [torch.tensor(emb_train @ sub) for sub in subspace]\n",
    "    sub_feat_train = torch.stack(sub_feat, dim = 1).reshape(emb_train.shape[0], -1)\n",
    "    \n",
    "    emb_test = get_word_embeddings(X_test.tolist())\n",
    "    sub_feat = [torch.tensor(emb_test @ sub) for sub in subspace]\n",
    "    sub_feat_test = torch.stack(sub_feat, dim = 1).reshape(emb_test.shape[0], -1)\n",
    "    \n",
    "    # print(sub_feat_train.shape, sub_feat_test.shape)\n",
    "    # sys.exit()\n",
    "\n",
    "    test_features = [extract_features(t, entity_vocab) for t in X_test.tolist()]\n",
    "    pos_features_test = torch.stack([x[0] for x in test_features], dim = 0)\n",
    "    dep_features_test = torch.stack([x[1] for x in test_features], dim = 0)\n",
    "    # pos_features = pos_features.unsqueeze(0)  # Add batch dim\n",
    "    # dep_indices = dep_indices.unsqueeze(0)  # Add batch dim\n",
    "\n",
    "    # train_features = torch.stack([extract_features(t) for t in X_train.tolist()], dim = 0)\n",
    "    # test_features = torch.stack([extract_features(t) for t in X_test.tolist()], dim = 0)\n",
    "\n",
    "    X_train = mask_entities(X_train)\n",
    "    X_test = mask_entities(X_test)\n",
    "\n",
    "    train_dataloader = get_dataset(X_train.tolist(), train_label.tolist(), pos_features_train, dep_features_train, sub_feat_train)\n",
    "    test_dataloader = get_dataset(X_test.tolist(), test_label.tolist(), pos_features_test, dep_features_test, sub_feat_test)\n",
    "\n",
    "    num_classes = len(set(train_label.tolist()))\n",
    "    model = CustomBERT(num_classes)\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(train_label.tolist())))\n",
    "    model.to(device)\n",
    "    'Model Loaded!'\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = 5e-5, eps = 1e-8, weight_decay=0.05)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    epochs = 100\n",
    "    num_training_steps = epochs * 8\n",
    "    lr_scheduler = get_scheduler(\n",
    "                name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "            )\n",
    "    loss_values = []\n",
    "    train_perf, test_perf = [], []\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        batch_loss_values = []\n",
    "\n",
    "        np.random.seed(epoch)\n",
    "        torch.manual_seed(epoch)\n",
    "        # print('Here')\n",
    "        torch.cuda.manual_seed_all(epoch)\n",
    "        # random.seed(epoch)\n",
    "        random_ids = np.random.randint(len(train_dataloader.dataset), size=512)\n",
    "        temp_train_dataloader = train_dataloader.dataset[random_ids]\n",
    "        # dataset = TensorDataset(temp_train_dataloader[0], temp_train_dataloader[1])\n",
    "        dataset = TensorDataset(temp_train_dataloader[0], temp_train_dataloader[1], temp_train_dataloader[2], temp_train_dataloader[3], temp_train_dataloader[4], temp_train_dataloader[5])\n",
    "        sampler = RandomSampler(dataset)\n",
    "        dataloader = DataLoader(dataset, batch_size=64, sampler = sampler)\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # emb = torch.tensor([emb])\n",
    "            input_ids, attention_mask, labels, feat1, feat2, sub_feat = [x.to(device) for x in batch]\n",
    "            # print(tok)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, feat1, feat2, sub_feat)\n",
    "\n",
    "            # logits = logits.squeeze(-1)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            reg_lambda = 0.00001\n",
    "            # l1_loss = sum([p.abs().sum() for p in model.parameters()])\n",
    "            l2_loss = sum([(p ** 2).sum() for p in model.parameters()])\n",
    "            loss = reg_lambda * l2_loss + loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            batch_loss_values.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        tr_perf = get_perf(train_dataloader)\n",
    "        tst_perf = get_perf(test_dataloader)\n",
    "\n",
    "        train_perf.append(tr_perf)\n",
    "        test_perf.append(tst_perf)\n",
    "\n",
    "        avg_batch_loss = sum(batch_loss_values)/len(batch_loss_values)\n",
    "\n",
    "        # print(f'Epoch {epoch} completed! Training Loss: {avg_batch_loss}. Training F1: {tr_perf} and Testing F1: {tst_perf}')\n",
    "    temp_df = pd.DataFrame({'train': train_perf, 'test': test_perf})\n",
    "    temp_df.to_csv(f'results/exp/bert_base_cased_label_{attr}.csv')\n",
    "    torch.save(model, f\"models/exp/bert_base_cased_label_{attr}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30473367-b55a-43ed-a30e-f828939318c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_df.sort_values('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692690f9-7a8e-4570-b652-83d71f4d349a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_df.sort_values('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f743f-ec9d-4edb-8f77-9e60d51f2af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_10",
   "language": "python",
   "name": "py3_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
