{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a3412b-b73c-4899-8f52-750e7d7aec3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The code is for benchmarking the war and peace journalism dataset with different language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba56cd-bd3a-4451-bfae-2e27317bb02f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Change the variable model_name to whichever model you wish to use. Ensure to make other relevant changes (some models may not require any change). \n",
    "### The required changes have to be made inside LanguageModel class, change self.model = '' to appropriate model initialization, and in intializing the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408a315-163c-49bc-8975-8b5743a47d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, ModernBertModel, T5Tokenizer, T5EncoderModel\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be56b8-fb10-4210-9703-b3b73b403bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, attr, siamese_training, num_classes = 2):\n",
    "        \n",
    "        super(LanguageModel, self).__init__()\n",
    "        torch.manual_seed(0)\n",
    "        # self.model = T5EncoderModel.from_pretrained(model_name)\n",
    "        # self.model = ModernBertModel.from_pretrained(model_name)\n",
    "        # self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        # self.model.load_state_dict(torch.load(f'models/siamese/con_headline_roberta_lab_{attr}.pth'))\n",
    "        \n",
    "        \n",
    "        # self.attention_vector = nn.Linear(self.model.config.hidden_size, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(p = 0.15)\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        \n",
    "        # x = self.model(input_ids = input_ids, attention_mask = attention_masks).last_hidden_state.mean(dim=1)\n",
    "        emb = self.model(input_ids = input_ids, attention_mask = attention_masks)\n",
    "        # hs = emb.hidden_states\n",
    "        # emb = torch.stack([x.mean(axis = 1) for x in hs]).mean(axis = 0)\n",
    "        # emb = emb.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        # attn_scores = self.attention_vector(emb.last_hidden_state).squeeze(-1)  # Shape: (batch, seq_len)\n",
    "        # attn_weights = F.softmax(attn_scores, dim=1)  # Normalize across tokens\n",
    "        # emb = (attn_weights.unsqueeze(-1) * emb.last_hidden_state).sum(dim=1) \n",
    "        \n",
    "        # emb, _ = emb.last_hidden_state.max(dim=1)\n",
    "        \n",
    "        emb = emb.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # last_hidden_state\n",
    "        # print(self.model.config.hidden_size)\n",
    "        # .pooler_output\n",
    "        # print(\"Here \", x.shape)\n",
    "        # sys.exit()\n",
    "        # .pooler_output\n",
    "        x = self.dropout(emb)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def get_dataset(sent, labels):\n",
    "    \n",
    "    tokens = tokenizer(sent, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # tokens = tokenizer(sent, return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    data = TensorDataset(tokens['input_ids'], tokens['attention_mask'], labels)\n",
    "\n",
    "    return DataLoader(data, batch_size = 64)\n",
    "\n",
    "def get_perf(dataloader):\n",
    "    \n",
    "    total_val_f1 = 0\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # emb = torch.tensor([emb])\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        # print(tok)\n",
    "        labels = labels.float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            # logits = model(input_ids)\n",
    "            \n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        total_val_f1 += f1_score(labels.cpu(), preds.cpu(), average = 'weighted')\n",
    "        \n",
    "    avg_val_f1 = total_val_f1 / len(dataloader)\n",
    "    \n",
    "    return avg_val_f1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbff1fd-275f-481c-a5da-80bc03a864ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def mask_entities(sentences: pd.Series) -> pd.Series:\n",
    "    def mask(sentence):\n",
    "        doc = nlp(sentence)\n",
    "        masked = sentence\n",
    "        for ent in doc.ents:\n",
    "            masked = masked.replace(ent.text, \"<mask>\")\n",
    "        return masked\n",
    "    return sentences.apply(mask)\n",
    "\n",
    "def get_data(attr):\n",
    "    \n",
    "    df = pd.read_csv(f'data/Label_{attr}.csv')\n",
    "    # print(df.columns)\n",
    "    \n",
    "    df['headline'] = mask_entities(df['headline'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd761674-b94d-4762-b7d0-b71d8f25fcb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d00e9-504f-4708-9cac-9d1110a8307b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_mapping = dict({'1':'emotive', '4a': 'effects', '4b': 'source', '4c': 'context', '3a': 'villain', '3b': 'victim'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c13f5-ee7a-4bec-ab53-96ab8cb4ee14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# model_name = 'bert-base-cased'\n",
    "model_name = 'roberta-base'\n",
    "# model_name = 'daodao/ConflictBERT'\n",
    "# model_name = 'snowood1/ConfliBERT-scr-cased'\n",
    "# model_name = 'facebook/bart-base'\n",
    "# model_name = 'google/gemma-2-9b'\n",
    "# model_name = 'studio-ousia/luke-base'\n",
    "# model_name = 'studio-ousia/mluke-base'\n",
    "# model_name = 'allenai/OLMoE-1B-7B-0924'\n",
    "# model_name = 'answerdotai/ModernBERT-base'\n",
    "# model_name = 'meta-llama/Llama-3.2-1B'\n",
    "# model_name = 't5-base'\n",
    "\n",
    "# from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "\n",
    "# Load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# siamese_training = 'verb_subspace'\n",
    "# 'head_cont_and_verb_subspace'\n",
    "# verb_subspace\n",
    "# 'simple_contrastive'\n",
    "\n",
    "# for siamese_training in ['verb_subspace', 'head_cont_and_verb_subspace', 'simple_contrastive']:\n",
    "\n",
    "siamese_training = None\n",
    "# attr = '1'\n",
    "# for attr in ['4a']:\n",
    "for attr in ['emotive', 'effects', 'source', 'context', 'villain', 'victim']:\n",
    "    # ['1', '4a', '4b']:\n",
    "    # \n",
    "    # for attr in ['1', '3a', '3b']:\n",
    "    data = get_data(attr)\n",
    "    X_train, X_test, train_label, test_label = train_test_split(data.headline, data.label, test_size=0.2, stratify = data.label, random_state = 42)\n",
    "\n",
    "    train_dataloader = get_dataset(X_train.tolist(), train_label.tolist())\n",
    "    test_dataloader = get_dataset(X_test.tolist(), test_label.tolist())\n",
    "\n",
    "    model = LanguageModel(attr, siamese_training, num_classes = len(set(train_label.tolist())))\n",
    "    # model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=len(set(train_label.tolist())))\n",
    "    # model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(train_label.tolist())))\n",
    "    # model.to(device)\n",
    "    model = nn.DataParallel(model).to(device)\n",
    "    \n",
    "    'Model Loaded!'\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = 5e-5, eps = 1e-8, weight_decay=0.05)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    epochs = 50\n",
    "    num_training_steps = epochs * 8\n",
    "    lr_scheduler = get_scheduler(\n",
    "                name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "            )\n",
    "    loss_values = []\n",
    "    train_perf, test_perf = [], []\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        batch_loss_values = []\n",
    "\n",
    "        np.random.seed(epoch)\n",
    "        torch.manual_seed(epoch)\n",
    "        torch.cuda.manual_seed_all(epoch)\n",
    "        # random.seed(epoch)\n",
    "        random_ids = np.random.randint(len(train_dataloader.dataset), size=512)\n",
    "        temp_train_dataloader = train_dataloader.dataset[random_ids]\n",
    "        # dataset = TensorDataset(temp_train_dataloader[0], temp_train_dataloader[1])\n",
    "        dataset = TensorDataset(temp_train_dataloader[0], temp_train_dataloader[1], temp_train_dataloader[2])\n",
    "        sampler = RandomSampler(dataset)\n",
    "        dataloader = DataLoader(dataset, batch_size=64, sampler = sampler)\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # emb = torch.tensor([emb])\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            # print(tok)\n",
    "            # labels = labels.float()\n",
    "\n",
    "            # outputs = bert(input_ids, attention_mask)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            # logits = model(input_ids)\n",
    "            # print(logits)\n",
    "            # sys.exit()\n",
    "\n",
    "            # logits = logits.squeeze(-1)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            reg_lambda = 0.00001\n",
    "            # l1_loss = sum([p.abs().sum() for p in model.parameters()])\n",
    "            l2_loss = sum([(p ** 2).sum() for p in model.parameters()])\n",
    "            loss = reg_lambda * l2_loss + loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            # print(\"Here \", loss) \n",
    "\n",
    "            batch_loss_values.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        tr_perf = get_perf(train_dataloader)\n",
    "        tst_perf = get_perf(test_dataloader)\n",
    "\n",
    "        train_perf.append(tr_perf)\n",
    "        test_perf.append(tst_perf)\n",
    "\n",
    "        avg_batch_loss = sum(batch_loss_values)/len(batch_loss_values)\n",
    "\n",
    "        # print(f'Epoch {epoch} completed! Training Loss: {avg_batch_loss}. Training F1: {tr_perf} and Testing F1: {tst_perf}')\n",
    "    temp_df = pd.DataFrame({'train': train_perf, 'test': test_perf})\n",
    "    temp_df.to_csv(f'results/benchmark/roberta-base/Label_{attr}_cls.csv')\n",
    "    torch.save(model, f\"models/benchmark/roberta-base/Label_{attr}_cls.pth\")\n",
    "\n",
    "    # torch.save(model.state_dict(), f\"models/roberta/Label_{attr}.pth\")\n",
    "\n",
    "    # temp_df.to_csv(f'results/siamese/{siamese_training}/roberta_Label_{attr}.csv')\n",
    "    # torch.save(model, f\"models/contrastive_models/{siamese_training}/roberta_Label_{attr}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f70980-e484-46bc-9b5b-365dd867b311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec13da-e2db-42c0-8203-73ea3d04e6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88395b59-1a0f-486f-a3cc-6d87a91dcdab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_10",
   "language": "python",
   "name": "py3_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
